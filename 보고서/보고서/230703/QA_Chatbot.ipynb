{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QA Chatbot\n",
        "# Structure\n",
        "\n",
        "<br/>*{개발중: 완료 X, 디벨롭 필요: 완성O[보수 필요], 주석없음: 완성 O}*\n",
        "<br/>chatbot\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ config\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ train_tools\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└ dict\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ utils\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ model\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;├ intent\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└ ner\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ test\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└ sample"
      ],
      "metadata": {
        "id": "-hjcIQJ9QKIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install konlpy\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU93PzKwSpKA",
        "outputId": "2d49fbc9-9ed3-4777-c1a1-1718e18172ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "-vViU35mRkLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ./config/GlobalPrams.py\n",
        "\n",
        "# 단어 시퀀스 벡터 크기\n",
        "MAX_SEQ_LEN = 25\n",
        "\n",
        "def GlobalParams():\n",
        "    \tglobal MAX_SEQ_LEN"
      ],
      "metadata": {
        "id": "IcIm-WndRmQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "iZ7IJ3XbSTR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess\n",
        "from konlpy.tag import Komoran\n",
        "import pickle\n",
        "\n",
        "class Preprocess:\n",
        "    # 생성자\n",
        "\tdef __init__(self, word2index_dic=\"\", userdic=None):\n",
        "\t\t# 단어 인덱스 사전 불러오기\n",
        "\t\tif word2index_dic != \"\":\n",
        "\t\t\tf = open(word2index_dic, \"rb\")\n",
        "\t\t\tself.word_index = pickle.load(f)\n",
        "\t\t\tf.close()\n",
        "\t\telse:\n",
        "\t\t\tself.word_index = None\n",
        "\n",
        "\t\t# 형태소 분석기 초기화\n",
        "\t\tself.komoran = Komoran(userdic=userdic)\n",
        "\n",
        "\t\t# 제외할 품사\n",
        "\t\t# 참조: https://docs.komoran.kr/firststep/postypes.html\n",
        "\t\tself.exclusion_tags = [\n",
        "            \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", # 용언 제거\n",
        "\t\t\t\"JKS\", \"JKC\", \"JKG\", \"JKO\", \"JKB\", \"JKV\", \"JKQ\", \"JX\", \"JC\", # 관계언 제거\n",
        "\t\t\t\"SF\", \"SP\", \"SS\", \"SE\", \"SO\", # 기호 제거\n",
        "\t\t\t\"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\", # 어미 제거\n",
        "\t\t\t\"XSN\", \"XSV\", \"XSA\", # 접미사 제거\n",
        "\t\t]\n",
        "\n",
        "\t# 형태소 분석기 POS tagger (래퍼 함수)\n",
        "\tdef pos(self, sentence):\n",
        "\t\treturn self.komoran.pos(sentence)\n",
        "\n",
        "\t# 불용어 제거 후 필요한 품사 정보만 가져오기\n",
        "\tdef get_keywords(self, pos, without_tag=False):\n",
        "\t\tf = lambda x: x in self.exclusion_tags\n",
        "\t\tword_list = []\n",
        "\t\tfor p in pos:\n",
        "\t\t\tif f(p[1]) is False: # 불용어 리스트에 없는 경우에만 저장\n",
        "\t\t\t\tword_list.append(p if without_tag is False else p[0])\n",
        "\t\treturn word_list\n",
        "\n",
        "\t# 키워드를 단어 인덱스 시퀀스로 변환\n",
        "\tdef get_wordidx_sequence(self, keywords):\n",
        "\t\tif self.word_index is None:\n",
        "\t\t\treturn []\n",
        "\t\tw2i = []\n",
        "\t\tfor word in keywords:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tw2i.append(self.word_index[word])\n",
        "\t\t\texcept KeyError:\n",
        "\t\t\t\t# 해당 단어가 사전에 없는 경우 OOV 처리\n",
        "\t\t\t\tw2i.append(self.word_index[\"OOV\"])\n",
        "\t\treturn w2i"
      ],
      "metadata": {
        "id": "H3PRSSTDSUG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d1CpiO67x2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cal_padding"
      ],
      "metadata": {
        "id": "vsHWXlItSXnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # 0 = all messages are logged (default behavior), 1 = INFO messages are not printed, 2 = INFO and WARNING messages are not printed, 3 = INFO, WARNING, and ERROR messages are not printed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "from utils.Preprocess import Preprocess\n",
        "\n",
        "data = pd.read_csv('./model/intent/train_data.csv')\n",
        "\n",
        "# 전처리 객체 생성\n",
        "p = Preprocess(userdic='./utils/user_dic.tsv')\n",
        "\n",
        "data_tokenized = [p.pos(text_) for text_ in data['query']]\n",
        "# pos = data_tokenized[0]\n",
        "# keywords = p.get_keywords(pos, without_tag=True)\n",
        "\n",
        "data_list = []\n",
        "for i in range(len(data_tokenized)):\n",
        "    pos = data_tokenized[i]\n",
        "    keywords = p.get_keywords(pos, without_tag=True)\n",
        "    data_list.append(keywords)\n",
        "\n",
        "num_tokens = [len(tokens) for tokens in data_list]\n",
        "num_tokens = np.array(num_tokens)\n",
        "\n",
        "# 평균값, 최댓값, 표준편차\n",
        "print(f\"토큰 길이 평균: {np.mean(num_tokens)}\")\n",
        "print(f\"토큰 길이 최대: {np.max(num_tokens)}\")\n",
        "print(f\"토큰 길이 표준편차: {np.std(num_tokens)}\")\n",
        "\n",
        "# plt.title('all text length')\n",
        "# plt.hist(num_tokens, bins=100)\n",
        "# plt.xlabel('length of samples')\n",
        "# plt.ylabel('number of samples')\n",
        "# plt.show()\n",
        "\n",
        "select_length = 17\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    cnt = 0\n",
        "    for s in nested_list:\n",
        "        if(len(s) <= max_len):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %0.2f%%'%(max_len, (cnt / len(nested_list) * 100)))\n",
        "\n",
        "below_threshold_len(select_length, data_list)"
      ],
      "metadata": {
        "id": "-mHoLo_iSZS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dict"
      ],
      "metadata": {
        "id": "RIr9uJu9SPTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creat_dict.py\n",
        "\n",
        "# 단어 사전 파일 생성\n",
        "# 챗봇에 사용하는 사전 파일\n",
        "\n",
        "import os\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "from utils.Preprocess import Preprocess\n",
        "from tensorflow.keras import preprocessing\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# 말뭉치 데이터 읽어오기\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "test_data.dropna(inplace=True)  # NaN 데이터 제거, inplace: 원본을 변경할지의 여부\n",
        "corpus_data = list(test_data['Query']) + list(test_data['Answer'])\n",
        "\n",
        "# 전처리 객체 생성\n",
        "p = Preprocess(userdic='./user_dic.tsv')\n",
        "\n",
        "# 말뭉치 데이터로부터 사전 리스트 생성\n",
        "dict = []\n",
        "for c in corpus_data:\n",
        "    pos = p.pos(c)\n",
        "    for k in pos:\n",
        "        dict.append(k[0])\n",
        "print(dict)\n",
        "\n",
        "# # 사전에 사용될 word2index 생성\n",
        "# # 사전의 첫 번째 인덱스에는 OVV 사용\n",
        "# tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')#, num_words=100000)\n",
        "# tokenizer.fit_on_texts(dict)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# # 사전 파일 생성\n",
        "# f = open(\"chatbot_dict_test_v0.1.1.bin\", \"wb\")\n",
        "# try:\n",
        "#     pickle.dump(word_index, f)\n",
        "# except Exception as e:\n",
        "#     print(e)\n",
        "# finally:\n",
        "#     f.close()\n",
        "\n",
        "# import json\n",
        "# with open('chatbot_dict_test_v0.1.1.json', 'w', encoding='utf-8') as f:\n",
        "#     json.dump(word_index, f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "tMZGag8ASPr3",
        "outputId": "ff031e84-d4f5-4322-b6d4-ad90c1cee124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-19a77929dae7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create_dict.py\n",
        "\n",
        "# 단어 사전 파일 생성\n",
        "# 챗봇에 사용하는 사전 파일\n",
        "\n",
        "import os\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "# from utils.Preprocess import Preprocess\n",
        "from tensorflow.keras import preprocessing\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 말뭉치 데이터 읽어오기\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "test_data.dropna(inplace=True)  # NaN 데이터 제거, inplace: 원본을 변경할지의 여부\n",
        "corpus_data = list(test_data['Query']) + list(test_data['Answer'])\n",
        "\n",
        "# 전처리 객체 생성\n",
        "p = Preprocess(userdic='user_dic.tsv')\n",
        "\n",
        "# 말뭉치 데이터로부터 사전 리스트 생성\n",
        "dict = []\n",
        "for c in corpus_data:\n",
        "    pos = p.pos(c)\n",
        "    for k in pos:\n",
        "        dict.append(k[0])\n",
        "\n",
        "num_tokens = [len(tokens) for tokens in dict]\n",
        "num_tokens = np.array(num_tokens)\n",
        "\n",
        "# 평균값, 최댓값, 표준편차\n",
        "print(f\"토큰 길이 평균: {np.mean(num_tokens)}\")\n",
        "print(f\"토큰 길이 최대: {np.max(num_tokens)}\")\n",
        "print(f\"토큰 길이 표준편차: {np.std(num_tokens)}\")\n",
        "\n",
        "# # 사전에 사용될 word2index 생성\n",
        "# # 사전의 첫 번째 인덱스에는 OVV 사용\n",
        "# tokenizer = preprocessing.text.Tokenizer(oov_token='OOV', num_words=100000)\n",
        "# tokenizer.fit_on_texts(dict)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# # 사전 파일 생성\n",
        "# f = open(\"chatbot_dict_test_v0.1.bin\", \"wb\")\n",
        "# try:\n",
        "#     pickle.dump(word_index, f)\n",
        "# except Exception as e:\n",
        "#     print(e)\n",
        "# finally:\n",
        "#     f.close()\n",
        "\n",
        "# import json\n",
        "# with open('chatbot_dict_test_v0.1.json', 'w', encoding='utf-8') as f:\n",
        "#     json.dump(word_index, f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "KuSI1_nMSP5j",
        "outputId": "849dc3bd-6061-41b9-a9bc-955f519da75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2faaa8e32519>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "import pickle\n",
        "\n",
        "# 말뭉치 데이터 읽어오기\n",
        "def read_corpus_data(filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "    data = data[1:] # 헤더 제거\n",
        "  return data\n",
        "\n",
        "\n",
        "# 말뭉치 데이터 가져오기\n",
        "corpus_data = read_corpus_data('corpus.txt')\n",
        "\n",
        "# 말뭉치 데이터에서 키워드만 추출해서 사전 리스트 생성\n",
        "p = Preprocess()\n",
        "dict= []\n",
        "for c in corpus_data:\n",
        "  pos = p.pos(c[1])\n",
        "  for k in pos:\n",
        "    dict.append(k[0])\n",
        "\n",
        "# 사전에 사용될 word2index 생성\n",
        "# 사전의 첫 번째 인덱스에는 OOV 사용\n",
        "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
        "tokenizer.fit_on_texts(dict)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#사전 파일 생성\n",
        "f = open(\"chatbot_dict.bin\", \"wb\")\n",
        "try:\n",
        "  pickle.dump(word_index, f)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "finally:\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "PRsFEDdNkOmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "jbkmGsBRRqFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "intent"
      ],
      "metadata": {
        "id": "2jQY86RGRtOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "\n",
        "# 의도 분류 모델 모듈\n",
        "class IntentModel:\n",
        "    def __init__(self, model_name, preprocess):\n",
        "\n",
        "        # 의도 클래스 별 레이블\n",
        "        self.labels = {0: \"공통\", 1: \"철도차량\", 2: \"철도용품\", 3: \"철도차량개조\"}\n",
        "\n",
        "        # 의도 분류 모델 불러오기\n",
        "        self.model = load_model(model_name)\n",
        "\n",
        "        # 챗봇 Preprocess 객체\n",
        "        self.p = preprocess\n",
        "\n",
        "\n",
        "    # 의도 클래스 예측\n",
        "    def predict_class(self, query):\n",
        "        # 형태소 분석\n",
        "        pos = self.p.pos(query)\n",
        "\n",
        "        # 문장내 키워드 추출(불용어 제거)\n",
        "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
        "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "        # 단어 시퀀스 벡터 크기\n",
        "        # from config.GlobalParams import MAX_SEQ_LEN\n",
        "\n",
        "        # 패딩처리\n",
        "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "        predict = self.model.predict(padded_seqs)\n",
        "        predict_class = tf.math.argmax(predict, axis=1)\n",
        "        return predict_class.numpy()[0]"
      ],
      "metadata": {
        "id": "vEgTvpReRq-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intent_Model_Train"
      ],
      "metadata": {
        "id": "dh2FOoKGScpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "from utils.Preprocess import Preprocess\n",
        "from config.GlobalParams import MAX_SEQ_LEN\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = './model/intent/train_data.csv'\n",
        "data = pd.read_csv(train_file)\n",
        "query = data['query'].tolist()\n",
        "intent = data['intent'].tolist()\n",
        "\n",
        "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict_test_v0.1.bin',\n",
        "               userdic='./utils/user_dic.tsv')\n",
        "\n",
        "# 단어 시퀀스 생성\n",
        "sequences = []\n",
        "for sentence in query:\n",
        "    pos = p.pos(sentence)\n",
        "    keywords = p.get_keywords(pos, without_tag=True)\n",
        "    seq = p.get_wordidx_sequence(keywords)\n",
        "    sequences.append(seq)\n",
        "\n",
        "# 단어 시퀀스 패딩\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
        "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intent))\n",
        "ds = ds.shuffle(len(query))\n",
        "\n",
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(2)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(2)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(1)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 3\n",
        "VOCAB_SIZE = len(p.word_index) + 1\n",
        "\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=3,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=4,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(4, name='logits')(dropout_hidden)\n",
        "predictions = Dense(4, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "#모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "# fit함수를 돌리면 이렇게 아래 출력창에 훈련하는 과정이 출력된다.\n",
        "# partial_x_train, partial_y_train / 트레이닝 데이터 중에서 훈련을 시키는 데이터 수\n",
        "# epoch / 반복 훈련 수\n",
        "# batch_size=512 / 512 번 마다 가중치를 업데이트\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "\n",
        "# 모델 평가(테스트 데이터 셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "\n",
        "\n",
        "# 모델 저장\n",
        "model.save('intent_model_v.0.1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "HpZ6IPNmSdb1",
        "outputId": "30fea1e2-96b6-48b7-cf2d-f1effd2f1093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7e2bbbac98f9>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalMaxPool1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalParams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJtj66vkSdmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ner"
      ],
      "metadata": {
        "id": "RNh3KFkxR5U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train.model\n",
        "import sys, os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))))\n",
        "# from utils.Preprocess import Preprocess\n",
        "\n",
        "# 학습 파일 불러오기\n",
        "def read_file(file_name):\n",
        "    sents = []\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for idx, l in enumerate(lines):\n",
        "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
        "                this_sent = []\n",
        "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
        "                continue\n",
        "            elif l[0] == '\\n':\n",
        "                sents.append(this_sent)\n",
        "            else:\n",
        "                this_sent.append(tuple(l.split()))\n",
        "    return sents\n",
        "\n",
        "#../../train_tools/dict/chatbot_dict_test_v0.1.bin\n",
        "#../../utils/user_dic.tsv\n",
        "\n",
        "p = Preprocess(word2index_dic='chatbot_dict.bin',\n",
        "               userdic='sample_user_dic.tsv')\n",
        "\n",
        "# 학습용 말뭉치 데이터를 불러옴\n",
        "corpus = read_file('ner_train.txt')\n",
        "\n",
        "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
        "sentences, tags = [], []\n",
        "for t in corpus:\n",
        "    tagged_sentence = []\n",
        "    sentence, bio_tag = [], []\n",
        "    for w in t:\n",
        "        tagged_sentence.append((w[1], w[3]))\n",
        "        sentence.append(w[1])\n",
        "        bio_tag.append(w[3])\n",
        "\n",
        "    sentences.append(sentence)\n",
        "    tags.append(bio_tag)\n",
        "\n",
        "\n",
        "print(\"샘플 크기 : \\n\", len(sentences))\n",
        "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
        "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
        "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
        "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
        "\n",
        "# 토크나이저 정의\n",
        "# p = preprocessing.text.Tokenizer(oov_token='OOV')\n",
        "# p.fit_on_texts(sentences)\n",
        "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
        "tag_tokenizer.fit_on_texts(tags)\n",
        "\n",
        "# 단어사전 및 태그 사전 크기\n",
        "vocab_size = len(p.word_index) + 1\n",
        "tag_size = len(tag_tokenizer.word_index) + 1\n",
        "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
        "print(\"단어 사전 크기 :\", vocab_size)\n",
        "\n",
        "# 학습용 단어 시퀀스 생성\n",
        "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
        "# x_train = p.texts_to_sequences(sentences)\n",
        "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
        "\n",
        "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
        "index_to_ner[0] = 'PAD'\n",
        "\n",
        "# 시퀀스 패딩 처리\n",
        "max_len = 40\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
        "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
        "                                                    test_size=.2,\n",
        "                                                    random_state=1234)\n",
        "\n",
        "# 출력 데이터를 one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
        "\n",
        "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
        "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
        "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
        "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
        "\n",
        "\n",
        "# 모델 정의 (Bi-LSTM)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
        "\n",
        "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
        "model.save('ner_model_test_v.0.2.h5')\n",
        "\n",
        "# 시퀀스를 NER 태그로 변환\n",
        "def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
        "    result = []\n",
        "    for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
        "        temp = []\n",
        "        for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
        "            pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
        "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
        "        result.append(temp)\n",
        "    return result\n",
        "\n",
        "# 테스트 데이터셋의 NER 예측\n",
        "y_predicted = model.predict(x_test)\n",
        "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
        "test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
        "\n",
        "# F1 스코어 계산을 위해 사용\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "print(classification_report(test_tags, pred_tags))\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "\n",
        "model = tf.keras.models.load_model('ner_model_test_v.0.2.h5')\n",
        "\n",
        "# 새로운 유형의 문장 NER 예측\n",
        "word_to_index = tag_tokenizer.word_index\n",
        "new_sentence = '오늘 13시 30분 짜장면 주문할게요.'\n",
        "pos = p.pos(new_sentence)\n",
        "keywords = p.get_keywords(pos, without_tag=True)\n",
        "new_seq = p.get_wordidx_sequence(keywords)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_seq)\n",
        "\n",
        "new_x = []\n",
        "for w in keywords:\n",
        "    try:\n",
        "        new_x.append(word_to_index.get(w, 1))\n",
        "    except KeyError:\n",
        "        new_x.append(word_to_index['OOV'])\n",
        "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x], padding='post', value=0, maxlen=max_len)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_x)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_padded_seqs)\n",
        "\n",
        "# from tensorflow.keras.models import Model, load_model\n",
        "# NER 예측\n",
        "# model = load_model('ner_model_test.h5')\n",
        "mp = model.predict(np.array([new_padded_seqs[0]]))\n",
        "mp = np.argmax(mp, axis=-1)   # 예측된 NER 인덱스값 추출\n",
        "\n",
        "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
        "print(\"-\"*50)\n",
        "for w, pred in zip(keywords, mp[0]):\n",
        "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZw-QUGkR61r",
        "outputId": "1818950c-af9f-4162-a309-ea72f4e4ff3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 크기 : \n",
            " 61999\n",
            "0번 째 샘플 단어 시퀀스 : \n",
            " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
            "0번 째 샘플 bio 태그 : \n",
            " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
            "샘플 단어 시퀀스 최대 길이 : 168\n",
            "샘플 단어 시퀀스 평균 길이 : 8.796238649010467\n",
            "BIO 태그 사전 크기 : 10\n",
            "단어 사전 크기 : 17751\n",
            "학습 샘플 시퀀스 형상 :  (49599, 40)\n",
            "학습 샘플 레이블 형상 :  (49599, 40, 10)\n",
            "테스트 샘플 시퀀스 형상 :  (12400, 40)\n",
            "테스트 샘플 레이블 형상 :  (12400, 40, 10)\n",
            "Epoch 1/10\n",
            "388/388 [==============================] - 381s 919ms/step - loss: 0.1131 - accuracy: 0.9684\n",
            "Epoch 2/10\n",
            "388/388 [==============================] - 366s 945ms/step - loss: 0.0384 - accuracy: 0.9875\n",
            "Epoch 3/10\n",
            "388/388 [==============================] - 364s 938ms/step - loss: 0.0256 - accuracy: 0.9916\n",
            "Epoch 4/10\n",
            "388/388 [==============================] - 358s 922ms/step - loss: 0.0204 - accuracy: 0.9931\n",
            "Epoch 5/10\n",
            "388/388 [==============================] - 359s 925ms/step - loss: 0.0172 - accuracy: 0.9943\n",
            "Epoch 6/10\n",
            "388/388 [==============================] - 359s 926ms/step - loss: 0.0149 - accuracy: 0.9951\n",
            "Epoch 7/10\n",
            "388/388 [==============================] - 359s 926ms/step - loss: 0.0136 - accuracy: 0.9953\n",
            "Epoch 8/10\n",
            "388/388 [==============================] - 357s 920ms/step - loss: 0.0135 - accuracy: 0.9953\n",
            "Epoch 9/10\n",
            "388/388 [==============================] - 361s 931ms/step - loss: 0.0123 - accuracy: 0.9957\n",
            "Epoch 10/10\n",
            "388/388 [==============================] - 358s 921ms/step - loss: 0.0121 - accuracy: 0.9959\n",
            "388/388 [==============================] - 25s 63ms/step - loss: 0.0598 - accuracy: 0.9862\n",
            "평가 결과 :  0.9861880540847778\n",
            "388/388 [==============================] - 25s 63ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_OG seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_DT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_TI seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_LC seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_FOOD seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_PS seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NP       1.00      1.00      1.00       303\n",
            "           _       0.55      0.53      0.54       658\n",
            "         _DT       1.00      1.00      1.00     13683\n",
            "       _FOOD       1.00      1.00      1.00     11655\n",
            "         _LC       0.78      0.52      0.62       314\n",
            "         _OG       0.57      0.55      0.56       460\n",
            "         _PS       0.60      0.50      0.54       396\n",
            "         _TI       0.80      0.70      0.75        61\n",
            "\n",
            "   micro avg       0.97      0.97      0.97     27530\n",
            "   macro avg       0.79      0.73      0.75     27530\n",
            "weighted avg       0.97      0.97      0.97     27530\n",
            "\n",
            "F1-score: 97.0%\n",
            "새로운 유형의 시퀀스 :  [40, 1, 1, 527, 3]\n",
            "새로운 유형의 시퀀스 :  [1, 1, 1, 1, 1]\n",
            "새로운 유형의 시퀀스 :  [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0]]\n",
            "1/1 [==============================] - 1s 888ms/step\n",
            "단어         예측된 NER\n",
            "--------------------------------------------------\n",
            "오늘         B_OG \n",
            "13시        O    \n",
            "30분        O    \n",
            "짜장면        O    \n",
            "주문         O    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ner model test\n",
        "import os, sys\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))))\n",
        "# from utils.Preprocess import Preprocess\n",
        "\n",
        "\n",
        "#../../train_tools/dict/chatbot_dict.bin\n",
        "#../../utils/user_dic.tsv\n",
        "\n",
        "p = Preprocess(word2index_dic='chatbot_dict.bin',\n",
        "               userdic='sample_user_dic.tsv')\n",
        "# 토크나이저 정의\n",
        "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
        "\n",
        "# 새로운 유형의 문장 NER 예측\n",
        "new_sentence = '무지개떡 주문 하고 싶어요'\n",
        "pos = p.pos(new_sentence)\n",
        "keywords = p.get_keywords(pos, without_tag=True)\n",
        "new_seq = p.get_wordidx_sequence(keywords)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_seq)\n",
        "\n",
        "new_padded_seqs = preprocessing.sequence.pad_sequences([new_seq], padding='post', value=0, maxlen=40)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_seq)\n",
        "print(\"새로운 유형의 시퀀스 : \", new_padded_seqs)\n",
        "\n",
        "# from tensorflow.keras.models import Model, load_model\n",
        "# NER 예측\n",
        "model = load_model('ner_model_test_v.0.2.h5')\n",
        "mp = model.predict(np.array([new_padded_seqs[0]]))\n",
        "mp = np.argmax(mp, axis=-1)   # 예측된 NER 인덱스값 추출\n",
        "\n",
        "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
        "print(\"-\"*50)\n",
        "index_to_ner = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', 6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD'}\n",
        "for w, pred in zip(keywords, mp[0]):\n",
        "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0qc82cOR8WS",
        "outputId": "969b1744-bab7-4878-d25b-ab4388884b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새로운 유형의 시퀀스 :  [1, 3]\n",
            "새로운 유형의 시퀀스 :  [1, 3]\n",
            "새로운 유형의 시퀀스 :  [[1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "단어         예측된 NER\n",
            "--------------------------------------------------\n",
            "무지개떡       B_FOOD\n",
            "주문         O    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "-TUwmSgISKnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test/model_intent_test.py\n",
        "\n",
        "# from utils.Preprocess import Preprocess\n",
        "# from model.intent.IntentModel import IntentModel\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#./train_tools/dict/chatbot_dict_test_v0.1.bin\n",
        "#./utils/user_dic.tsv\n",
        "\n",
        "p = Preprocess(word2index_dic='chatbot_dict_test_v.0.1.bin',\n",
        "               userdic='user_dic.tsv')\n",
        "\n",
        "#./model/intent/intent_model_v.0.1.h5\n",
        "intent = IntentModel(model_name='intent_model_v.0.1.h5', preprocess=p)\n",
        "\n",
        "#./model/intent/train_data.csv\n",
        "train_file = 'train_data.csv'\n",
        "data = pd.read_csv(train_file)\n",
        "query = data['query'].tolist()\n",
        "\n",
        "# for sentence in query:\n",
        "#     predict = intent.predict_class(sentence)\n",
        "#     predict_label = intent.labels[predict]\n",
        "#     print(\"=\"*30)\n",
        "#     print(sentence)\n",
        "#     print(\"의도 예측 클래스 : \", predict)\n",
        "#     print(\"의도 예측 레이블 : \", predict_label)\n",
        "\n",
        "query = \"미입회로 형식시험을 진행할 수 있나요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]\n",
        "# print(query, predict_label)\n",
        "print(\"=\"*30)\n",
        "print(query)\n",
        "print(\"의도 예측 클래스 : \", predict)\n",
        "print(\"의도 예측 레이블 : \", predict_label)\n",
        "\n",
        "query = \"동일한 철도차량의 계약 건마다 새로운 형식승인을 받아야 하나요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]\n",
        "print(\"=\"*30)\n",
        "print(query)\n",
        "print(\"의도 예측 클래스 : \", predict)\n",
        "print(\"의도 예측 레이블 : \", predict_label)\n",
        "\n",
        "query = \"철도용품 입찰에 참여하기 위해서는 형식승인과 제작자승인을 모두 받아야 하며 비용은 제작자가 부담해야 하나요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]\n",
        "print(\"=\"*30)\n",
        "print(query)\n",
        "print(\"의도 예측 클래스 : \", predict)\n",
        "print(\"의도 예측 레이블 : \", predict_label)\n",
        "\n",
        "query = \"운행승인 신청시 준비해야할 안전성 검토자료는 구체적으로 무엇인가요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]\n",
        "print(\"=\"*30)\n",
        "print(query)\n",
        "print(\"의도 예측 클래스 : \", predict)\n",
        "print(\"의도 예측 레이블 : \", predict_label)\n",
        "\n",
        "query = \"철도용품 기술기준 고시 후에 종전 구매한 유지보수품은 사용가능한가요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]\n",
        "print(\"=\"*30)\n",
        "print(query)\n",
        "print(\"의도 예측 클래스 : \", predict)\n",
        "print(\"의도 예측 레이블 : \", predict_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky30EhQlSLM5",
        "outputId": "022e12a5-c60f-4e4e-aa42-821245af3fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 193ms/step\n",
            "==============================\n",
            "미입회로 형식시험을 진행할 수 있나요?\n",
            "의도 예측 클래스 :  0\n",
            "의도 예측 레이블 :  공통\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "==============================\n",
            "동일한 철도차량의 계약 건마다 새로운 형식승인을 받아야 하나요?\n",
            "의도 예측 클래스 :  0\n",
            "의도 예측 레이블 :  공통\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "==============================\n",
            "철도용품 입찰에 참여하기 위해서는 형식승인과 제작자승인을 모두 받아야 하며 비용은 제작자가 부담해야 하나요?\n",
            "의도 예측 클래스 :  2\n",
            "의도 예측 레이블 :  철도용품\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "==============================\n",
            "운행승인 신청시 준비해야할 안전성 검토자료는 구체적으로 무엇인가요?\n",
            "의도 예측 클래스 :  3\n",
            "의도 예측 레이블 :  철도차량개조\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "==============================\n",
            "철도용품 기술기준 고시 후에 종전 구매한 유지보수품은 사용가능한가요?\n",
            "의도 예측 클래스 :  2\n",
            "의도 예측 레이블 :  철도용품\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Jc6t1OLGCN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER_Model_Module"
      ],
      "metadata": {
        "id": "jACzMSX3GD9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "\n",
        "# 개체명 인식 모델 모듈\n",
        "class NerModel:\n",
        "  def __init__(self, model_name, preprocess):\n",
        "    self.index_to_ner  = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', 6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD'}\n",
        "\n",
        "    # 의도 분류 모델 불러오기\n",
        "    self.model = load_model(model_name)\n",
        "\n",
        "    #챗봇 Preprocess 객체\n",
        "    self.p = preprocess\n",
        "\n",
        "  #개체명 클래스 예측\n",
        "  def predict(self, query):\n",
        "    #형태소 분석\n",
        "    pos = self.p.pos(query)\n",
        "\n",
        "    #문장 내 키워드 추출(불용어 제거)\n",
        "    keywords = self.p.get_keywords(pos, without_tag=True)\n",
        "    sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "    #패딩 처리\n",
        "    max_len = 40\n",
        "    padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding='post', value=0, maxlen=max_len)\n",
        "\n",
        "    #키워드별 개체명 예측\n",
        "    predict = self.model.predict(np.array([padded_seqs[0]]))\n",
        "    predict_class = tf.math.argmax(predict, axis=-1)\n",
        "\n",
        "    tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
        "    return list(zip(keywords, tags))\n",
        "\n",
        "  def predict_tags(self, query):\n",
        "    #형태소 분석\n",
        "    pos = self.p.pos(query)\n",
        "\n",
        "    #문장 내 키워드 추출(불용어 제거)\n",
        "    keywords = self.p.get_keywords(pos, without_tag=True)\n",
        "    sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "    # 패딩 처리\n",
        "    max_len = 40\n",
        "    padded_seqs = preprocessing.seqence.pad_sequences(sequences, padding='post', value=0, max_len=max_len)\n",
        "\n",
        "    predict = self.model.predict(np.array([padded_seqs[0]]))\n",
        "    predict_class = tf.math.argmax(predict, axis=-1)\n",
        "\n",
        "    tags = []\n",
        "    for tag_idx in predict_class.numpy()[0]:\n",
        "      if tag_idx == 1: continue\n",
        "      tags.append(self.index_to_ner[tag_idx])\n",
        "\n",
        "      if len(tags) == 0:\n",
        "        return None\n",
        "      return tags"
      ],
      "metadata": {
        "id": "RBn6i31uGGLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nermodel 객체 사용\n",
        "\n",
        "p = Preprocess(word2index_dic='chatbot_dict.bin',\n",
        "               userdic='sample_user_dic.tsv')\n",
        "\n",
        "ner = NerModel(model_name='ner_model_test_v.0.2.h5', preprocess=p)\n",
        "query = \"고구마 튀김\"\n",
        "predicts = ner.predict(query)\n",
        "print(predicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYI35U94KmFK",
        "outputId": "4f972ce5-faa8-436c-df6a-a6d3e05c039e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 893ms/step\n",
            "[('고구마', 'O'), ('튀김', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alYnzInKLTCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 답변 검색"
      ],
      "metadata": {
        "id": "L1FlPT5MLTWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터베이스 제어 모듈"
      ],
      "metadata": {
        "id": "Ncniu8WqLzjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql\n",
        "!pip install logging"
      ],
      "metadata": {
        "id": "fIMM7g9HL1xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import pymysql.cursors\n",
        "import logging\n",
        "\n",
        "\n",
        "class Database:\n",
        "  '''\n",
        "  데이터베이스 제어\n",
        "  '''\n",
        "  def __init__(self, host, user, password, db_name, charset='utf8'):\n",
        "    self.host = host\n",
        "    self.user = user\n",
        "    self.password = password\n",
        "    self.charset = charset\n",
        "    self.db_name = db_name\n",
        "    self.conn = None\n",
        "\n",
        "  # DB 연결\n",
        "  def connect(self):\n",
        "    if self.conn != None:\n",
        "      return\n",
        "    self.conn = pymysql.connect(\n",
        "        host=self.host,\n",
        "        user=self.user,\n",
        "        password=self.password,\n",
        "        db=self.db_name,\n",
        "        charset=self.charset,\n",
        "    )\n",
        "\n",
        "  # DB 연결 닫기\n",
        "  def close(self):\n",
        "    if self.conn is None:\n",
        "      return\n",
        "    if not self.conn.open:\n",
        "      self.conn = None\n",
        "      return\n",
        "    self.conn.close()\n",
        "    self.conn = None\n",
        "\n",
        "  # SQL 구문 실행\n",
        "  def execute(self, sql):\n",
        "    last_row_id = -1\n",
        "    try:\n",
        "      with self.conn.cursor() as cursor:\n",
        "        cursor.execute(sql)\n",
        "      self.conn.commit()\n",
        "      lasst_row_id = cursor.lastrowid\n",
        "    except Exception as ex:\n",
        "      logging.error(ex)\n",
        "\n",
        "    finally:\n",
        "      return last_row_id\n",
        "\n",
        "  # SELECT 구문 실행 후 단 1개의 데이터 ROW만 불러옴\n",
        "  def select_one(self, sql):\n",
        "    result = None\n",
        "    try:\n",
        "      with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
        "        cursor.execute(sql)\n",
        "        result = cursor.fetchone()\n",
        "    except Exception as ex:\n",
        "      logging.error(ex)\n",
        "\n",
        "    finally:\n",
        "      return result\n",
        "\n",
        "  # SELECT 구문 실해 후 전체 데이터 ROW를 불러옴\n",
        "  def select_all(self, sql):\n",
        "    result = None\n",
        "\n",
        "    try:\n",
        "      with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
        "        cursor.execute(sql)\n",
        "        result = cursor.fetchall()\n",
        "    except Exception as ex:\n",
        "      logging.error(ex)\n",
        "\n",
        "    finally:\n",
        "      return result"
      ],
      "metadata": {
        "id": "xnaj-oYNLUKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗봇 답변 검색 모듈"
      ],
      "metadata": {
        "id": "Tv3Gz58KNpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FindAnswer:\n",
        "  def __init__(self, db):\n",
        "    self.db = db\n",
        "\n",
        "  # 검색 쿼리 생성\n",
        "  def _make_query(self, intent_name, ner_tags):\n",
        "    sql = \"select * from chatbot_train_data\"\n",
        "    if intent_name != None and ner_tags == None:\n",
        "      sql = sql + f\" where intent={intent_name}\"\n",
        "\n",
        "    elif intent_name != None and ner_tags != None:\n",
        "      where = f' where intent={intent_name}'\n",
        "      if (len(ner_tags) > 0):\n",
        "        where += 'and ('\n",
        "        for ne in ner_tags:\n",
        "          were += f' ner like {ne} or'\n",
        "        where = where[:-3] + ')'\n",
        "      sql = sql + where\n",
        "\n",
        "    # 동일한 답변이 2개 이상인 경우 랜덤으로 선택\n",
        "    sql = sql + \" order by rand() limit 1\"\n",
        "    return sql\n",
        "\n",
        "\n",
        "  # 답변 검색\n",
        "  def search(self, intent_name, ner_tags):\n",
        "    #의도명과 개체명으로 답변 검색\n",
        "    sql = self._make_query(intent_name, ner_tags)\n",
        "    answer = self.db.select_one(sql)\n",
        "\n",
        "    #검색되는 답변이 없으면 의도명만 검색\n",
        "    if answer is None:\n",
        "      sql = self._make_query(intent_name, None)\n",
        "      answer = self.db.select_one(sql)\n",
        "\n",
        "    return (answer['answer'], answer['answer_image'])\n",
        "\n",
        "  # NER 태그를 실제 입력된 단어로 변환\n",
        "  def tag_to_word(self, ner_predicts, answer):\n",
        "    for word, tag in ner_predicts:\n",
        "      # 변환해야 하는 태그가 있는 경우 추가\n",
        "      if tag == 'B_FOOD':\n",
        "        answer = answer.replace(tag, word)\n",
        "\n",
        "    answer = answer.replace('{', '')\n",
        "    answer = answer.replace('}', '')\n",
        "    return answer"
      ],
      "metadata": {
        "id": "8-50zw40Nr8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_t01eP8SCme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#챗봇 엔진 동작"
      ],
      "metadata": {
        "id": "_djMzuyMSDC1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30kS9y1jSEEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}